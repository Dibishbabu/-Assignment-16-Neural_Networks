{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Neural networks/gas_turbines.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX  \n",
       "0      82.722  \n",
       "1      82.776  \n",
       "2      82.468  \n",
       "3      82.670  \n",
       "4      82.311  \n",
       "...       ...  \n",
       "15034  79.559  \n",
       "15035  79.917  \n",
       "15036  90.912  \n",
       "15037  93.227  \n",
       "15038  92.498  \n",
       "\n",
       "[15039 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "df1 = df.drop('TEY',axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>TEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "      <td>114.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "      <td>114.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "      <td>111.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "      <td>111.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "      <td>110.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "      <td>110.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "      <td>111.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX     TEY  \n",
       "0      82.722  114.70  \n",
       "1      82.776  114.72  \n",
       "2      82.468  114.71  \n",
       "3      82.670  114.72  \n",
       "4      82.311  114.72  \n",
       "...       ...     ...  \n",
       "15034  79.559  111.61  \n",
       "15035  79.917  111.78  \n",
       "15036  90.912  110.19  \n",
       "15037  93.227  110.74  \n",
       "15038  92.498  111.58  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = pd.concat([pd.DataFrame(df1),\n",
    "                     df[['TEY']]], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114.7 ],\n",
       "       [114.72],\n",
       "       [114.71],\n",
       "       ...,\n",
       "       [110.19],\n",
       "       [110.74],\n",
       "       [111.58]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "array = finalDf.values\n",
    "X = array[:,0:10]\n",
    "Y = array[:,10]\n",
    "\n",
    "X.reshape(-1,1)\n",
    "Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10,  activation='relu'))\n",
    "model.add(Dense(8,  activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 2/150\n",
      "1053/1053 [==============================] - 1s 911us/step - loss: 18324.2617 - val_loss: 17209.2656\n",
      "Epoch 3/150\n",
      "1053/1053 [==============================] - 1s 826us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 4/150\n",
      "1053/1053 [==============================] - 1s 861us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 5/150\n",
      "1053/1053 [==============================] - 1s 929us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 6/150\n",
      "1053/1053 [==============================] - 1s 828us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 7/150\n",
      "1053/1053 [==============================] - 1s 837us/step - loss: 18324.2480 - val_loss: 17209.2656\n",
      "Epoch 8/150\n",
      "1053/1053 [==============================] - 1s 836us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 9/150\n",
      "1053/1053 [==============================] - 1s 845us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 10/150\n",
      "1053/1053 [==============================] - 1s 877us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 11/150\n",
      "1053/1053 [==============================] - 1s 864us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 12/150\n",
      "1053/1053 [==============================] - 1s 883us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 13/150\n",
      "1053/1053 [==============================] - 1s 873us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 14/150\n",
      "1053/1053 [==============================] - 1s 845us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 15/150\n",
      "1053/1053 [==============================] - 1s 891us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 16/150\n",
      "1053/1053 [==============================] - 1s 886us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 17/150\n",
      "1053/1053 [==============================] - 1s 879us/step - loss: 18324.2637 - val_loss: 17209.2656\n",
      "Epoch 18/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2344 - val_loss: 17209.2656\n",
      "Epoch 19/150\n",
      "1053/1053 [==============================] - 1s 900us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 20/150\n",
      "1053/1053 [==============================] - 1s 861us/step - loss: 18324.2363 - val_loss: 17209.2656\n",
      "Epoch 21/150\n",
      "1053/1053 [==============================] - 1s 848us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 22/150\n",
      "1053/1053 [==============================] - 1s 875us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 23/150\n",
      "1053/1053 [==============================] - 1s 855us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 24/150\n",
      "1053/1053 [==============================] - 1s 845us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 25/150\n",
      "1053/1053 [==============================] - 1s 840us/step - loss: 18324.2637 - val_loss: 17209.2656\n",
      "Epoch 26/150\n",
      "1053/1053 [==============================] - 1s 858us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 27/150\n",
      "1053/1053 [==============================] - 1s 879us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 28/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 29/150\n",
      "1053/1053 [==============================] - 1s 969us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 30/150\n",
      "1053/1053 [==============================] - 1s 968us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 31/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2402 - val_loss: 17209.2656\n",
      "Epoch 32/150\n",
      "1053/1053 [==============================] - 1s 917us/step - loss: 18324.2344 - val_loss: 17209.2656\n",
      "Epoch 33/150\n",
      "1053/1053 [==============================] - 1s 884us/step - loss: 18324.2637 - val_loss: 17209.2656\n",
      "Epoch 34/150\n",
      "1053/1053 [==============================] - 1s 874us/step - loss: 18324.2305 - val_loss: 17209.2656\n",
      "Epoch 35/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 36/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2617 - val_loss: 17209.2656\n",
      "Epoch 37/150\n",
      "1053/1053 [==============================] - 1s 954us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 38/150\n",
      "1053/1053 [==============================] - 1s 938us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 39/150\n",
      "1053/1053 [==============================] - 1s 870us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 40/150\n",
      "1053/1053 [==============================] - 1s 998us/step - loss: 18324.2480 - val_loss: 17209.2656\n",
      "Epoch 41/150\n",
      "1053/1053 [==============================] - 1s 855us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 42/150\n",
      "1053/1053 [==============================] - 1s 849us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 43/150\n",
      "1053/1053 [==============================] - 1s 850us/step - loss: 18324.2695 - val_loss: 17209.2656\n",
      "Epoch 44/150\n",
      "1053/1053 [==============================] - 1s 854us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 45/150\n",
      "1053/1053 [==============================] - 1s 864us/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 46/150\n",
      "1053/1053 [==============================] - 1s 954us/step - loss: 18324.2324 - val_loss: 17209.2656\n",
      "Epoch 47/150\n",
      "1053/1053 [==============================] - 1s 838us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 48/150\n",
      "1053/1053 [==============================] - 1s 899us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 49/150\n",
      "1053/1053 [==============================] - 1s 878us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 50/150\n",
      "1053/1053 [==============================] - 1s 900us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 51/150\n",
      "1053/1053 [==============================] - 1s 986us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 52/150\n",
      "1053/1053 [==============================] - 1s 944us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 53/150\n",
      "1053/1053 [==============================] - 1s 855us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 54/150\n",
      "1053/1053 [==============================] - 1s 872us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 55/150\n",
      "1053/1053 [==============================] - 1s 914us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 56/150\n",
      "1053/1053 [==============================] - 1s 933us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 57/150\n",
      "1053/1053 [==============================] - 1s 933us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 58/150\n",
      "1053/1053 [==============================] - 1s 950us/step - loss: 18324.2480 - val_loss: 17209.2656\n",
      "Epoch 59/150\n",
      "1053/1053 [==============================] - 1s 887us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 60/150\n",
      "1053/1053 [==============================] - 1s 880us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 61/150\n",
      "1053/1053 [==============================] - 1s 852us/step - loss: 18324.2402 - val_loss: 17209.2656\n",
      "Epoch 62/150\n",
      "1053/1053 [==============================] - 1s 843us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 63/150\n",
      "1053/1053 [==============================] - 1s 849us/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 64/150\n",
      "1053/1053 [==============================] - 1s 875us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 65/150\n",
      "1053/1053 [==============================] - 1s 801us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 66/150\n",
      "1053/1053 [==============================] - 1s 770us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 67/150\n",
      "1053/1053 [==============================] - 1s 901us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 68/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 69/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 70/150\n",
      "1053/1053 [==============================] - 1s 887us/step - loss: 18324.2617 - val_loss: 17209.2656\n",
      "Epoch 71/150\n",
      "1053/1053 [==============================] - 1s 870us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 72/150\n",
      "1053/1053 [==============================] - 1s 840us/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 73/150\n",
      "1053/1053 [==============================] - 1s 820us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 74/150\n",
      "1053/1053 [==============================] - 1s 987us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 75/150\n",
      "1053/1053 [==============================] - 1s 905us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 76/150\n",
      "1053/1053 [==============================] - 1s 939us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 77/150\n",
      "1053/1053 [==============================] - 1s 931us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 78/150\n",
      "1053/1053 [==============================] - 1s 869us/step - loss: 18324.2344 - val_loss: 17209.2656\n",
      "Epoch 79/150\n",
      "1053/1053 [==============================] - 1s 874us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 80/150\n",
      "1053/1053 [==============================] - 1s 800us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 81/150\n",
      "1053/1053 [==============================] - 1s 800us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 82/150\n",
      "1053/1053 [==============================] - 1s 985us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 83/150\n",
      "1053/1053 [==============================] - 1s 909us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 84/150\n",
      "1053/1053 [==============================] - 1s 908us/step - loss: 18324.2617 - val_loss: 17209.2656\n",
      "Epoch 85/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 86/150\n",
      "1053/1053 [==============================] - 1s 927us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 87/150\n",
      "1053/1053 [==============================] - 1s 860us/step - loss: 18324.2656 - val_loss: 17209.2656\n",
      "Epoch 88/150\n",
      "1053/1053 [==============================] - 1s 995us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 89/150\n",
      "1053/1053 [==============================] - 1s 955us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 90/150\n",
      "1053/1053 [==============================] - 1s 868us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 91/150\n",
      "1053/1053 [==============================] - 1s 950us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 92/150\n",
      "1053/1053 [==============================] - 1s 888us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 93/150\n",
      "1053/1053 [==============================] - 1s 833us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 94/150\n",
      "1053/1053 [==============================] - 1s 828us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 95/150\n",
      "1053/1053 [==============================] - 1s 829us/step - loss: 18324.2363 - val_loss: 17209.2656\n",
      "Epoch 96/150\n",
      "1053/1053 [==============================] - 1s 836us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 97/150\n",
      "1053/1053 [==============================] - 1s 848us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 98/150\n",
      "1053/1053 [==============================] - 1s 847us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 99/150\n",
      "1053/1053 [==============================] - 1s 839us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 100/150\n",
      "1053/1053 [==============================] - 1s 835us/step - loss: 18324.2598 - val_loss: 17209.2656\n",
      "Epoch 101/150\n",
      "1053/1053 [==============================] - 1s 828us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 102/150\n",
      "1053/1053 [==============================] - 1s 945us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 103/150\n",
      "1053/1053 [==============================] - 1s 992us/step - loss: 18324.2500 - val_loss: 17209.2656\n",
      "Epoch 104/150\n",
      "1053/1053 [==============================] - 1s 853us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 105/150\n",
      "1053/1053 [==============================] - 1s 854us/step - loss: 18324.2676 - val_loss: 17209.2656\n",
      "Epoch 106/150\n",
      "1053/1053 [==============================] - 1s 835us/step - loss: 18324.2734 - val_loss: 17209.2656\n",
      "Epoch 107/150\n",
      "1053/1053 [==============================] - 1s 879us/step - loss: 18324.2559 - val_loss: 17209.2656\n",
      "Epoch 108/150\n",
      "1053/1053 [==============================] - 1s 834us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 109/150\n",
      "1053/1053 [==============================] - 1s 827us/step - loss: 18324.2676 - val_loss: 17209.2656\n",
      "Epoch 110/150\n",
      "1053/1053 [==============================] - 1s 837us/step - loss: 18324.2305 - val_loss: 17209.2656\n",
      "Epoch 111/150\n",
      "1053/1053 [==============================] - 1s 845us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 112/150\n",
      "1053/1053 [==============================] - 1s 842us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 113/150\n",
      "1053/1053 [==============================] - 1s 840us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 114/150\n",
      "1053/1053 [==============================] - 1s 834us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 115/150\n",
      "1053/1053 [==============================] - 1s 836us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 116/150\n",
      "1053/1053 [==============================] - 1s 842us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 117/150\n",
      "1053/1053 [==============================] - 1s 830us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 118/150\n",
      "1053/1053 [==============================] - 1s 852us/step - loss: 18324.2363 - val_loss: 17209.2656\n",
      "Epoch 119/150\n",
      "1053/1053 [==============================] - 1s 844us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 120/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 121/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18324.2402 - val_loss: 17209.2656\n",
      "Epoch 122/150\n",
      "1053/1053 [==============================] - 1s 899us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 123/150\n",
      "1053/1053 [==============================] - 1s 887us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 124/150\n",
      "1053/1053 [==============================] - 1s 830us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 125/150\n",
      "1053/1053 [==============================] - 1s 842us/step - loss: 18324.2637 - val_loss: 17209.2656\n",
      "Epoch 126/150\n",
      "1053/1053 [==============================] - 1s 902us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 127/150\n",
      "1053/1053 [==============================] - 1s 858us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 128/150\n",
      "1053/1053 [==============================] - 1s 848us/step - loss: 18324.2363 - val_loss: 17209.2656\n",
      "Epoch 129/150\n",
      "1053/1053 [==============================] - 1s 835us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 130/150\n",
      "1053/1053 [==============================] - 1s 878us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 131/150\n",
      "1053/1053 [==============================] - 1s 863us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 132/150\n",
      "1053/1053 [==============================] - 1s 854us/step - loss: 18324.2402 - val_loss: 17209.2656\n",
      "Epoch 133/150\n",
      "1053/1053 [==============================] - 1s 855us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 134/150\n",
      "1053/1053 [==============================] - 1s 886us/step - loss: 18324.2656 - val_loss: 17209.2656\n",
      "Epoch 135/150\n",
      "1053/1053 [==============================] - 1s 922us/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 136/150\n",
      "1053/1053 [==============================] - 1s 784us/step - loss: 18324.2617 - val_loss: 17209.2656\n",
      "Epoch 137/150\n",
      "1053/1053 [==============================] - 1s 907us/step - loss: 18324.2363 - val_loss: 17209.2656\n",
      "Epoch 138/150\n",
      "1053/1053 [==============================] - 1s 923us/step - loss: 18324.2324 - val_loss: 17209.2656\n",
      "Epoch 139/150\n",
      "1053/1053 [==============================] - 1s 846us/step - loss: 18324.2539 - val_loss: 17209.2656\n",
      "Epoch 140/150\n",
      "1053/1053 [==============================] - 1s 846us/step - loss: 18324.2422 - val_loss: 17209.2656\n",
      "Epoch 141/150\n",
      "1053/1053 [==============================] - 1s 845us/step - loss: 18324.2715 - val_loss: 17209.2656\n",
      "Epoch 142/150\n",
      "1053/1053 [==============================] - 1s 835us/step - loss: 18324.2461 - val_loss: 17209.2656\n",
      "Epoch 143/150\n",
      "1053/1053 [==============================] - 1s 847us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 144/150\n",
      "1053/1053 [==============================] - 1s 827us/step - loss: 18324.2480 - val_loss: 17209.2656\n",
      "Epoch 145/150\n",
      "1053/1053 [==============================] - 1s 808us/step - loss: 18324.2578 - val_loss: 17209.2656\n",
      "Epoch 146/150\n",
      "1053/1053 [==============================] - 1s 821us/step - loss: 18324.2383 - val_loss: 17209.2656\n",
      "Epoch 147/150\n",
      "1053/1053 [==============================] - 1s 816us/step - loss: 18324.2520 - val_loss: 17209.2656\n",
      "Epoch 148/150\n",
      "1053/1053 [==============================] - 1s 819us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 149/150\n",
      "1053/1053 [==============================] - 1s 835us/step - loss: 18324.2441 - val_loss: 17209.2656\n",
      "Epoch 150/150\n",
      "1053/1053 [==============================] - 1s 806us/step - loss: 18324.2500 - val_loss: 17209.2656\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 584us/step - loss: 17989.7305\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
